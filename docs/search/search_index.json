{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Axcent - AI Agent Framework The easiest way to build AI agents in Python. Axcent is a lightweight framework designed to let you build powerful AI agents with tool calling, multimodal support (images & audio), context caching, and multi-backend support in just a few lines of code. Installation pip install axcent Quick Start from axcent import Agent agent = Agent(system_prompt=\"You are a helpful assistant.\") response = agent.ask(\"Hello!\") print(response)","title":"Home"},{"location":"#axcent-ai-agent-framework","text":"The easiest way to build AI agents in Python. Axcent is a lightweight framework designed to let you build powerful AI agents with tool calling, multimodal support (images & audio), context caching, and multi-backend support in just a few lines of code.","title":"Axcent - AI Agent Framework"},{"location":"#installation","text":"pip install axcent","title":"Installation"},{"location":"#quick-start","text":"from axcent import Agent agent = Agent(system_prompt=\"You are a helpful assistant.\") response = agent.ask(\"Hello!\") print(response)","title":"Quick Start"},{"location":"api/","text":"API Reference axcent.core.Agent The main class for interacting with the LLM. __init__ def __init__(self, system_prompt: str = \"...\", backend: LLMBackend = None, model: str = \"gpt-4o-mini\") system_prompt : The initial instruction for the agent. backend : An instance of LLMBackend (default: OpenAIBackend ). model : The model name string (used if backend is default). ask def ask(self, query: str, media: Optional[List[Media]] = None) -> str Sends a user query to the agent with optional media attachments. Handles the conversation loop including tool execution. tool @agent.tool def my_function(): ... Decorator to register a function as a tool. get_total_usage def get_total_usage() -> Dict[str, int] Returns a dictionary containing token usage statistics (prompt, completion, total, cached). axcent.llm OpenAIBackend Standard backend for OpenAI-compatible APIs with vision support. GeminiBackend Backend for Google's Gemini models using google-genai SDK. Supports multimodal inputs. MockBackend Backend for testing without API calls. axcent.media Image Image(path: str = None, url: str = None) Wrapper for image content. Provide either a local path or a url . Audio Audio(path: str = None, url: str = None) Wrapper for audio content. Provide either a local path or a url . Transcriber Transcriber(system_prompt: str, backend: LLMBackend = None) Converts media (images/audio) to text using an LLM. transcribe(url: str) -> str : Transcribe media from URL transcribe_file(path: str) -> str : Transcribe media from local file","title":"API Reference"},{"location":"api/#api-reference","text":"","title":"API Reference"},{"location":"api/#axcentcoreagent","text":"The main class for interacting with the LLM.","title":"axcent.core.Agent"},{"location":"api/#__init__","text":"def __init__(self, system_prompt: str = \"...\", backend: LLMBackend = None, model: str = \"gpt-4o-mini\") system_prompt : The initial instruction for the agent. backend : An instance of LLMBackend (default: OpenAIBackend ). model : The model name string (used if backend is default).","title":"__init__"},{"location":"api/#ask","text":"def ask(self, query: str, media: Optional[List[Media]] = None) -> str Sends a user query to the agent with optional media attachments. Handles the conversation loop including tool execution.","title":"ask"},{"location":"api/#tool","text":"@agent.tool def my_function(): ... Decorator to register a function as a tool.","title":"tool"},{"location":"api/#get_total_usage","text":"def get_total_usage() -> Dict[str, int] Returns a dictionary containing token usage statistics (prompt, completion, total, cached).","title":"get_total_usage"},{"location":"api/#axcentllm","text":"","title":"axcent.llm"},{"location":"api/#openaibackend","text":"Standard backend for OpenAI-compatible APIs with vision support.","title":"OpenAIBackend"},{"location":"api/#geminibackend","text":"Backend for Google's Gemini models using google-genai SDK. Supports multimodal inputs.","title":"GeminiBackend"},{"location":"api/#mockbackend","text":"Backend for testing without API calls.","title":"MockBackend"},{"location":"api/#axcentmedia","text":"","title":"axcent.media"},{"location":"api/#image","text":"Image(path: str = None, url: str = None) Wrapper for image content. Provide either a local path or a url .","title":"Image"},{"location":"api/#audio","text":"Audio(path: str = None, url: str = None) Wrapper for audio content. Provide either a local path or a url .","title":"Audio"},{"location":"api/#transcriber","text":"Transcriber(system_prompt: str, backend: LLMBackend = None) Converts media (images/audio) to text using an LLM. transcribe(url: str) -> str : Transcribe media from URL transcribe_file(path: str) -> str : Transcribe media from local file","title":"Transcriber"},{"location":"getting-started/","text":"Getting Started Basic Usage The core of Axcent is the Agent class. import os from axcent import Agent # 1. Set your API Key os.environ[\"OPENAI_API_KEY\"] = \"sk-...\" # 2. Initialize Agent agent = Agent(system_prompt=\"You are a helpful assistant.\") # 3. Ask a question response = agent.ask(\"What is the capital of France?\") print(response) Using Tools Axcent makes it incredibly easy to give your agent tools. Just define a python function and use the @agent.tool decorator. @agent.tool def get_weather(city: str) -> str: \"\"\"Get the current weather for a city.\"\"\" # In a real app, you'd call an API here return f\"The weather in {city} is sunny!\" response = agent.ask(\"What's the weather in Tokyo?\") print(response) The Docstring and Type Hints are automatically converted to the JSON schema required by the LLM. Multi-Backend Support Axcent supports multiple LLM providers. Google Gemini from axcent import Agent, GeminiBackend import os os.environ[\"GEMINI_API_KEY\"] = \"AIza...\" # Use Gemini Backend (V2) backend = GeminiBackend(model=\"gemini-2.0-flash-exp\") agent = Agent(system_prompt=\"You are a helper.\", backend=backend) OpenRouter import os from axcent import Agent os.environ[\"OPENAI_API_KEY\"] = \"sk-or-...\" os.environ[\"OPENAI_BASE_URL\"] = \"https://openrouter.ai/api/v1\" agent = Agent(system_prompt=\"You are a helper.\") Multimodal: Images & Audio Axcent supports multimodal inputs through the Transcriber class. Using Transcriber as a Tool from axcent import Agent, Transcriber from axcent.llm import GeminiBackend agent = Agent(system_prompt=\"You are a helpful assistant.\") @agent.tool def see_media(path: str) -> str: \"\"\"Analyze an image or audio file.\"\"\" transcriber = Transcriber( system_prompt=\"Describe this media briefly.\", backend=GeminiBackend() ) return transcriber.transcribe_file(path) response = agent.ask(\"What's in /home/user/photo.jpg?\") Direct Media with ask() from axcent import Agent, Image agent = Agent(model=\"gpt-4o\") # Vision model img = Image(url=\"https://example.com/photo.jpg\") # Or from file: img = Image(path=\"/path/to/image.jpg\") response = agent.ask(\"What's in this image?\", media=[img])","title":"Getting Started"},{"location":"getting-started/#getting-started","text":"","title":"Getting Started"},{"location":"getting-started/#basic-usage","text":"The core of Axcent is the Agent class. import os from axcent import Agent # 1. Set your API Key os.environ[\"OPENAI_API_KEY\"] = \"sk-...\" # 2. Initialize Agent agent = Agent(system_prompt=\"You are a helpful assistant.\") # 3. Ask a question response = agent.ask(\"What is the capital of France?\") print(response)","title":"Basic Usage"},{"location":"getting-started/#using-tools","text":"Axcent makes it incredibly easy to give your agent tools. Just define a python function and use the @agent.tool decorator. @agent.tool def get_weather(city: str) -> str: \"\"\"Get the current weather for a city.\"\"\" # In a real app, you'd call an API here return f\"The weather in {city} is sunny!\" response = agent.ask(\"What's the weather in Tokyo?\") print(response) The Docstring and Type Hints are automatically converted to the JSON schema required by the LLM.","title":"Using Tools"},{"location":"getting-started/#multi-backend-support","text":"Axcent supports multiple LLM providers.","title":"Multi-Backend Support"},{"location":"getting-started/#google-gemini","text":"from axcent import Agent, GeminiBackend import os os.environ[\"GEMINI_API_KEY\"] = \"AIza...\" # Use Gemini Backend (V2) backend = GeminiBackend(model=\"gemini-2.0-flash-exp\") agent = Agent(system_prompt=\"You are a helper.\", backend=backend)","title":"Google Gemini"},{"location":"getting-started/#openrouter","text":"import os from axcent import Agent os.environ[\"OPENAI_API_KEY\"] = \"sk-or-...\" os.environ[\"OPENAI_BASE_URL\"] = \"https://openrouter.ai/api/v1\" agent = Agent(system_prompt=\"You are a helper.\")","title":"OpenRouter"},{"location":"getting-started/#multimodal-images-audio","text":"Axcent supports multimodal inputs through the Transcriber class.","title":"Multimodal: Images &amp; Audio"},{"location":"getting-started/#using-transcriber-as-a-tool","text":"from axcent import Agent, Transcriber from axcent.llm import GeminiBackend agent = Agent(system_prompt=\"You are a helpful assistant.\") @agent.tool def see_media(path: str) -> str: \"\"\"Analyze an image or audio file.\"\"\" transcriber = Transcriber( system_prompt=\"Describe this media briefly.\", backend=GeminiBackend() ) return transcriber.transcribe_file(path) response = agent.ask(\"What's in /home/user/photo.jpg?\")","title":"Using Transcriber as a Tool"},{"location":"getting-started/#direct-media-with-ask","text":"from axcent import Agent, Image agent = Agent(model=\"gpt-4o\") # Vision model img = Image(url=\"https://example.com/photo.jpg\") # Or from file: img = Image(path=\"/path/to/image.jpg\") response = agent.ask(\"What's in this image?\", media=[img])","title":"Direct Media with ask()"}]}